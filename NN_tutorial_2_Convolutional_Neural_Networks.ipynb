{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Crash Course on Neural Networks with Keras Part 2 - Convolutional Neural Networks (CNN's)\n",
    "\n",
    "Remaining within the supervised learning scenario, it is useful to explore some more sophisticated neural network architectures.\n",
    "\n",
    "In particular, *Convolutional Neural Networks* have become a standard and powerful tool for many problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a) What are CNN's, and why use them?\n",
    "\n",
    "Very roughly, convolutional neural networks try to take advantage of:\n",
    "\n",
    "   - local\n",
    "   - translation invariant\n",
    "      \n",
    "features in patterns/data/images.\n",
    "\n",
    "For example, we can probably classify an image as a face, just by knowing that somewhere in the image is an eye, somewhere else a mouth and somewhere else a nose - i.e. local information/patterns/features can be very useful, regardless of where in the image/global pattern it occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we design a neural network that can do this? An answer is to use convolutional filters! \n",
    "\n",
    "The following explanation, and all pictures, are taken from this [great blog post](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/) - this explanation will be significantly more brief, so for details and discussion I suggest reading there, and [here](http://cs231n.github.io/convolutional-networks/) :)\n",
    "\n",
    "To get an idea of the ingredients, lets start off with an extremely simple example of a 1D pattern (eg: some time series).\n",
    "\n",
    "<img src=\"images/1d_conv.png\",width=400,height=400>\n",
    "\n",
    "In the above example, instead of having a fully-connected layer of different neurons generating the activations after the first layer, we can imagine *a single consistent neuron* (labelled A in the diagram) applied succesively to neighbouring data points. This individual neuron then detects some specific nearest neighbour feature - and the output after this first layer, created by sweeping this individual neuron across the pattern, encodes where in the time series this specific nearest neighbour feature can be found. In the jargon, this single neuron is called a *filter*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we are not limited to nearest neighbour features, we could for example use a filter that detects \"2-local\" features:\n",
    "\n",
    "<img src=\"images/1d_conv_2.png\",width=400,height=400>\n",
    "\n",
    "And we can of course stack these things on top of each other to build deep-networks extracting a hierachy of succesive features:\n",
    "\n",
    "<img src=\"images/1d_conv_3.png\",width=400,height=400>\n",
    "\n",
    "Importantly, we can \"coarse-grain\" by introducing a pooling operation, which asks whether a feature is present in an enlarged area of the image/pattern/time series:\n",
    "\n",
    "<img src=\"images/1d_conv_4.png\",width=400,height=400>\n",
    "\n",
    "Now, of course we can extend this to 2D inputs (i.e. greyscale images):\n",
    "\n",
    "<img src=\"images/2d_conv.png\",width=400,height=400>\n",
    "\n",
    "And finally, to build a full convolutional neural network, we can imagine having multiple filters at each level:\n",
    "\n",
    "<img src=\"images/CNN_complete.png\",width=600,height=600>\n",
    "\n",
    "<img src=\"images/CNN_stride.gif\",width=400,height=400>\n",
    "\n",
    "(great gif taken from [here](http://www.jessicayung.com/explaining-tensorflow-code-for-a-convolutional-neural-network/))\n",
    "\n",
    "Again, the hope is that by stacking these layers, we can learn a hierachy of succesively abstract features. For example, in  a typical ImageNet CNN the filters in the first layer often end up looking along these lines:\n",
    "\n",
    "<img src=\"images/filters.png\",width=600,height=600>\n",
    "\n",
    "This can be tricky to wrap your head around the first time you see it - what's visualized above is the weights leading into each of the filters in the first convolutional layer - each square corresponds to a different filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b) Some practical considerations\n",
    "\n",
    "When building convolutional neural networks, there are various things we need to specify:\n",
    "\n",
    "   - number of filters at each layer\n",
    "   - width and height of the convolutional kernel\n",
    "   - stride size\n",
    "   - padding methodology\n",
    "   \n",
    "For example, in the below image the kernel has width=height=3, the stride is 2, and there is a single pixel of zero-padding:\n",
    "   \n",
    "<img src=\"images/padding.gif\",width=400,height=400>\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Great, lets build a CNN in Keras\n",
    "\n",
    "As in the feed-forward case, building and training CNN's in Keras is extremely easy. To illustrate, we will use MNIST again.\n",
    "\n",
    "To illustrate some of the different ways you can do things in Keras, we will use the Sequential class here over the Model class, along with some other tricks :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Imports -----\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU \n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape and scale the data. Note that for a 2D convolutional layer Keras expects the input with shape:\n",
    "\n",
    "[batch_size, width, height, num_channels]\n",
    "\n",
    "In general, when using any Keras layer, the first thing to check is the expected input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train/=255\n",
    "X_test/=255\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we want to convert the labels to one-hot encodings - Keras actually has a built-in method for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_classes = 10\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, number_of_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, number_of_classes)\n",
    "\n",
    "y_train[0], Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we can go ahead and build the model.\n",
    "\n",
    "As mentioned before, the Sequential class is even easier than the Model class - you can simply build up a model by sequentially adding layers on top of each other, without having to explicitly specify the inputs to a new layer. \n",
    "\n",
    "This works under the assumption that the input to a layer is the output of the one previously added, which is often the case. However, sometimes we want to build more  sophisticated or branching networks, and its in these instances where the Sequential class wouldn't be good enough, and you would want to use the Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start off by specifying that our model is a Sequential object\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first conv layer:\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28,28,1)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We also use batch normalization between conv layers - see here https://arxiv.org/abs/1502.03167\n",
    "BatchNormalization(axis=-1)\n",
    "\n",
    "# Another conv layer\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Max Pooling (Coarse Graining)\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# etc...\n",
    "BatchNormalization(axis=-1)\n",
    "model.add(Conv2D(64,(3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "BatchNormalization(axis=-1)\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Finally, we flatten the output of the last conv layer, so that it can be fed into fully connected layers...\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers - a FFNN on top of the conv layers\n",
    "BatchNormalization()\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "BatchNormalization()\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "\n",
    "# We use a softmax as the final activation (ensures a legitimate probability distribution)\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 594,922\n",
      "Trainable params: 594,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have to compile the model specifying loss and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in this case we will do training slightly differently.\n",
    "\n",
    "The idea is to create an augmented data set to increase the number of training examples, and generate this data on the fly with a Generator object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                         height_shift_range=0.08, zoom_range=0.08)\n",
    "\n",
    "test_gen = ImageDataGenerator()\n",
    "\n",
    "train_generator = gen.flow(X_train, Y_train, batch_size=64)\n",
    "test_generator = test_gen.flow(X_test, Y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit the model on examples generated on the fly by the generator. Specifying batch size etc is a little more complicated in this case:\n",
    "\n",
    "NB: This is just to illustrate a different way of doing things! One could just called model.fit as in the FFNN case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "937/937 [==============================] - 9s 10ms/step - loss: 0.2061 - acc: 0.9343 - val_loss: 0.0311 - val_acc: 0.9889\n",
      "Epoch 2/5\n",
      "937/937 [==============================] - 8s 9ms/step - loss: 0.0630 - acc: 0.9805 - val_loss: 0.0310 - val_acc: 0.9902\n",
      "Epoch 3/5\n",
      "937/937 [==============================] - 8s 9ms/step - loss: 0.0484 - acc: 0.9853 - val_loss: 0.0178 - val_acc: 0.9937\n",
      "Epoch 4/5\n",
      "937/937 [==============================] - 8s 9ms/step - loss: 0.0417 - acc: 0.9869 - val_loss: 0.0179 - val_acc: 0.9948\n",
      "Epoch 5/5\n",
      "937/937 [==============================] - 8s 9ms/step - loss: 0.0363 - acc: 0.9886 - val_loss: 0.0133 - val_acc: 0.9953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1c9d2b080>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=60000//64, epochs=5, \n",
    "                    validation_data=test_generator, validation_steps=10000//64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can extract the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (3, 3, 1, 32)\n",
      "1 (32,)\n",
      "2 (3, 3, 32, 32)\n",
      "3 (32,)\n",
      "4 (3, 3, 32, 64)\n",
      "5 (64,)\n",
      "6 (3, 3, 64, 64)\n",
      "7 (64,)\n",
      "8 (1024, 512)\n",
      "9 (512,)\n",
      "10 (512, 10)\n",
      "11 (10,)\n"
     ]
    }
   ],
   "source": [
    "for count, weight in enumerate(weights):\n",
    "    print(count, weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally the filters are viewed analyzed by just looking at the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1c92a90b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_width = 8\n",
    "display_height = 4\n",
    "\n",
    "fig, axes1 = plt.subplots(display_height,display_width,figsize=(15,5))\n",
    "counter = 0\n",
    "for j in range(display_height):\n",
    "    for k in range(display_width):\n",
    "        axes1[j][k].set_axis_off()\n",
    "        axes1[j][k].imshow(weights[0][:,:,0,counter], cmap=\"binary\")\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently however there has been interesting work on trying to figure out what sort of features would activate what\n",
    "features - i.e. can we see how convnets view the world or better interpret what these features are learning?\n",
    "\n",
    "See [here](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html) and [here](https://distill.pub/2017/feature-visualization/) for a really great discussion with lots of insight (Thanks Paul for the tip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Beyond Convolutional Neural Networks:\n",
    "\n",
    "It's worth noting that very recently (NIPS 2017) the idea of [capsule networks](https://arxiv.org/abs/1710.09829) has been proposed. These are generalizations of convolutional neural networks, but which are capable of encoding the relative position of local features in a much more sophisticated way!\n",
    "\n",
    "These look extremely promising, and clean/informative Keras code can be found [here](https://github.com/XifengGuo/CapsNet-Keras) (Although, definitely read the paper first!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
