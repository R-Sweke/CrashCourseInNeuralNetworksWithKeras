{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Crash Course on Neural Networks with Keras Part 3 - Recurrent Neural Networks\n",
    "\n",
    "So far we have seen convnets, which are great at dealing with images via the extraction of translation invariant local features.\n",
    "\n",
    "In the context of sequence modelling recurrent neural networks (and in particular Long Short Term Memory (LSTM) networks) have become very popular, and so it's also worth having a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1a) What are RNN's, and why use them?\n",
    "\n",
    "This explanation is going to be an extremely brief summary of this truly excellent [blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (that I have taken all the images from). This is just to give a flavour, and to point out their existence, but I strongly suggest having a look at the linked blog to get more details and intuition!\n",
    "\n",
    "Very roughly, recurrent neural networks aim to gain an advantage in sequence modelling by at every time step in the sequence feeding into the network both the new time step, and the previous output of the network (i.e. by having recurrent connections). \n",
    "\n",
    "<center><img src=\"images/RNN-rolled.png\",width=200,height=200><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea is that one output of the cell should contain a representation/memory that is useful for cells seeing future information, and another output of the cell should be useful for any further processing at this time step. This is sometimes easier to see in an \"unrolled\" picture, but remember that the cells are identical (i.e. they share weights, in a way similar to convolutional filters):\n",
    "\n",
    "<center><img src=\"images/RNN-unrolled.png\",width=600,height=600><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are many different types of [recurrent cells](https://keras.io/layers/recurrent/), but arguably the most popular is the Long Short Term Memory (LSTM) cell:\n",
    "\n",
    "<center><img src=\"images/LSTM3-chain.png\",width=600,height=600><center>\n",
    "\n",
    "I don't want to go into the details - see [here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1b) A quick note on some practical considerations:\n",
    "\n",
    "Historically LSTM's have been the go-to tool for sequence modelling. However, they are expensive to train, often difficult to tune, and very recently it seems like consensus is shifting towards 1D convnets (as we discussed as an example CNN) being more robust, efficient and effective for sequence modelling:\n",
    "\n",
    "<center><img src=\"images/wavenet.gif\",width=600,height=600><center>\n",
    "\n",
    "Google's [\"WaveNet\"](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) above is a great example of this, and a very recent discussion (arXiv last week) can be found [here](https://arxiv.org/abs/1803.01271).\n",
    "\n",
    "So, the moral is, perhaps this section is more for historical interest - things are changing fast :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2) Lets build an LSTM\n",
    "\n",
    "For this example we will use the [IMDB sentiment dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification). Its a collection of movie reviews, labelled with a binary label representing positive or negative sentiment, and the idea is to predict the sentiment from an unlabelled review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# ----- Imports --------\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each movie review is encoded as a string of integers, where the integer represents the words, which are indexed by overall frequency in the dataset.\n",
    "\n",
    "So, as preprocessing we need to:\n",
    "\n",
    "   - decide how many words we will keep (less frequent words will all be allocated 0)\n",
    "   - the maximum number of words we want to keep from each review (i.e. how long will our sequence be)\n",
    "   \n",
    "We choose to retain only the top 20000 words, and to keep only the last 80 words of the review if the review is longer than that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Of course, by construction, LSTM's can be made to work with datasets consisting of sequences of varying length (see [Dynamic RNN](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) in TensorFlow).\n",
    "\n",
    "However currently Keras supports only datasets consisting of fixed length sequences, so we have to pad the sequences which are shorter than 80 words, and truncate the ones that are longer. Again, Keras has built in functionality for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "At this point, a single sequence (truncated review) looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can build the model.\n",
    "\n",
    "Unfortuntely, integer encodings of words would make *extremely* bad features (ideally, we want features to be normalized with mean 0 variance 1 - see earlier discussion and references) - as a result, we need our first layer to be a word [embedding layer](https://keras.io/layers/embeddings/), which learns a vector representation of the words.\n",
    "\n",
    "In other situations, where each element of the sequence is naturally given as a vector of features, we could start with the LSTM layer, which expects its input in the shape:\n",
    "\n",
    "[batch_size, sequence_length, num_features]\n",
    "\n",
    "Note here also that we only have a single LSTM layer, and that by default these layers only return the output from the final LSTM cell (indicated by return_sequences=False) which we then push into a feed forward layer. \n",
    "\n",
    "In principal though we can stack many LSTM layers on top of each other (extracting sequences of sucessively more abstract features), but to make this work you have to set return_sequences=True on all intermediate LSTM layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And, once again, training is now easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 69s 3ms/step - loss: 0.4591 - acc: 0.7829 - val_loss: 0.3953 - val_acc: 0.8288\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 70s 3ms/step - loss: 0.3003 - acc: 0.8780 - val_loss: 0.3688 - val_acc: 0.8374\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 71s 3ms/step - loss: 0.2165 - acc: 0.9155 - val_loss: 0.4566 - val_acc: 0.8210\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 70s 3ms/step - loss: 0.1554 - acc: 0.9413 - val_loss: 0.4431 - val_acc: 0.8310\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 69s 3ms/step - loss: 0.1115 - acc: 0.9586 - val_loss: 0.6299 - val_acc: 0.8229\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 69s 3ms/step - loss: 0.0779 - acc: 0.9721 - val_loss: 0.6400 - val_acc: 0.8212\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 70s 3ms/step - loss: 0.0592 - acc: 0.9798 - val_loss: 0.7850 - val_acc: 0.8056\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 71s 3ms/step - loss: 0.0440 - acc: 0.9853 - val_loss: 0.7793 - val_acc: 0.8140\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 71s 3ms/step - loss: 0.0294 - acc: 0.9902 - val_loss: 0.8802 - val_acc: 0.8143\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 70s 3ms/step - loss: 0.0302 - acc: 0.9906 - val_loss: 0.9285 - val_acc: 0.8128\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 69s 3ms/step - loss: 0.0204 - acc: 0.9935 - val_loss: 0.9324 - val_acc: 0.8171\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 70s 3ms/step - loss: 0.0161 - acc: 0.9954 - val_loss: 1.0061 - val_acc: 0.8177\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 69s 3ms/step - loss: 0.0165 - acc: 0.9948 - val_loss: 1.1178 - val_acc: 0.8144\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 69s 3ms/step - loss: 0.0137 - acc: 0.9956 - val_loss: 1.0171 - val_acc: 0.8131\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 67s 3ms/step - loss: 0.0080 - acc: 0.9975 - val_loss: 1.0828 - val_acc: 0.8125\n",
      "25000/25000 [==============================] - 12s 484us/step\n",
      "Test score: 1.0828352694833279\n",
      "Test accuracy: 0.81252\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
